{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Population and Parameter.\n",
    "A **population** is any large collection of objects or individuals, such as Americans, students, or trees about which information is desired.\n",
    "\n",
    "A **parameter** is any summary number, like an average or percentage, that describes the entire population.\n",
    "\n",
    ">The population mean $\\mu$ and the population proportion p are two different population parameters\n",
    "\n",
    ">The problem is that 99.999999999999... % of the time, we don't — or can't — know the real value of a population parameter. The best we can do is estimate the parameter!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample and Statistic\n",
    "A **sample** is a representative group drawn from the population.\n",
    "\n",
    "A **statistic** is any summary number, like an average or percentage, that describes the sample.\n",
    "\n",
    "\n",
    "|         |Statistic from Sample        |Parameter from Population|\n",
    "|:------------ |:-------------|:-----|\n",
    "| Average      | $\\bar{x}$ <br> is the avg. revenue from a random sample of 100 customers | $\\mu$ <br>is the avg. revenue from all customers |\n",
    "| Proportion      | $\\hat{p}$ <br>is the proportion in a random sample of 100 customers who are loyal      |   p <br> is the proportion of all loyal customers |\n",
    "|Use-case  | Because samples are manageable in size, we can determine the actual value of any statistic      |    We use the known value of the sample statistic to learn about the unknown value of the population parameter.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Empirical Rule\n",
    "The standard deviation is the way variability is often described. If the data we are working with are roughly bell-shaped and symmetrical (looking like a normal distribution) then we can use the standard deviation to tell approximately how much of the data will cluster around the mean. \n",
    "\n",
    "\n",
    "The so called empirical rule states that the bulk of the data cluster around the mean in a normal distribution. In fact:\n",
    "- 68% of values fall within $ \\pm 1 $  standard deviation of the mean\n",
    "- 95% fall within $ \\pm 2 $  standard deviations of the mean\n",
    "- 99% fall within  $\\pm 3 $ standard deviations of the mean\n",
    "\n",
    "It's called the empirical rule since experimenters have observed roughly these patterns from their data over and over again when they empirically collect data.\n",
    "\n",
    "# Z-Score\n",
    "The z-score is just a fancy name for standard deviations. So a z-score of 2 is like saying 2 standard deviations above and below the the mean. A z-score of 1.5 is 1.5 standard deviations above and below the mean. A z-score of 0 is no standard deviations above or below the mean (it's equal to the mean). \n",
    "\n",
    "\n",
    "# Bias\n",
    "Bias is the tendency of a statistic to overestimate or underestimate a parameter.\n",
    "> Bias can seep into your results for a slew of reasons including sampling or measurement errors, or unrepresentative samples.\n",
    "mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "\n",
    "In statistics, estimation refers to the process by which one makes inferences about a population, based on information obtained from a sample.\n",
    "\n",
    "**Point estimate** A point estimate of a population parameter is a single value of a statistic. \n",
    ">For example, the sample mean $\\bar{x}$ is a point estimate of the population mean $\\mu$. \n",
    "\n",
    ">Similarly, the sample proportion p is a point estimate of the population proportion P.\n",
    "\n",
    "**Interval estimate** An interval estimate is defined by two numbers, between which a population parameter is said to lie. \n",
    ">For example, a < $\\bar{x}$ < b is an interval estimate of the population mean $\\mu$. \n",
    "\n",
    "It indicates that the population mean is greater than 'a' but less than 'b'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence interval & level\n",
    "- Confidence interval (CI) is a type of interval estimate which is *likely* to include an unknown population parameter,\n",
    "\n",
    "$CI = \\bar{X} \\pm z*\\dfrac{\\sigma}{\\sqrt(n)}$\n",
    "\n",
    ">z is the value from the standard normal distribution for the selected confidence level \n",
    "(e.g., for a 95% confidence level, z=1.96)\n",
    "\n",
    "# t-interval\n",
    "The population standard deviation is usually not known, (if we knew it, we would likely also know the population average $\\mu$, and have no need for an interval estimate.)\n",
    "\n",
    "Hence we calcualte T-interval for sample. \n",
    "\n",
    "$CI = \\bar{X} \\pm z*\\dfrac{SD}{\\sqrt(n)}$\n",
    "\n",
    "# Margin of Error. \n",
    "It can be used whenever samples are taken and an estimate is made about a larger population. The margin of error is half the confidence interval. The smaller the sample, the more variable the responses will be and the bigger the margin of error. \n",
    "\n",
    "# Effect of Population variability\n",
    "As the variability of the population you're sampling from increases the confidence interval of your sample gets wider. \n",
    "\n",
    "Statisticians use summary measures to describe the amount of variability or spread in a set of data. The most common measures of variability are the range, the interquartile range (IQR), variance, and standard deviation.\n",
    "\n",
    "\n",
    "#  Coefficient of variation ($C_v$)\n",
    "$C_v = \\dfrac{\\sigma}{\\mu}$\n",
    " - Also known as relative standard deviation (RSD)\n",
    " - Is a standardized measure of dispersion and is unitless\n",
    " \n",
    "The higher the coefficient of variation, the greater the level of dispersion around the mean. It is generally expressed as a percentage. Without units, it allows for comparison between distributions of values whose scales of measurement are not comparable.\n",
    "\n",
    "> For example, the expression “The standard deviation is 15% of the mean” is a CV. The CV is particularly useful when you want to compare results from two different surveys or tests that have different measures or values. For example, if you are comparing the results from two tests that have different scoring mechanisms.\n",
    "\n",
    "\n",
    "# Is Standard Deviation high for my data?\n",
    "Standard deviations aren't \"good\" or \"bad\". They are indicators of how spread out your data is. Sometimes, in ratings scales, we want wide spread because it indicates that our questions/ratings cover the range of the group we are rating. Other times, we want a small sd because we want everyone to be \"high\".\n",
    "\n",
    "- Min Order Value is Rs.8 \n",
    "- Max Order Value is Rs.284\n",
    "- Average Order Value is Rs.28 \n",
    "- Standard Deviation - Rs.27\n",
    "- CV - 96.4%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "- A hypothesis test is a statistical test that is used to determine whether there is enough evidence in a sample of data to infer that a certain condition is true for the entire population.\n",
    "- A hypothesis test examines two opposing hypotheses about a population: the null hypothesis and the alternative hypothesis. \n",
    "- **The null hypothesis** ($H_0$) is the statement being tested. Usually the null hypothesis is a statement of \"no effect\" or \"no difference\". \n",
    "- **The alternative hypothesis** ($H_1$ or $H_a$) is the statement you want to be able to conclude is true.\n",
    "\n",
    "# Critical value approach\n",
    "- The critical value approach involves determining \"likely\" or \"unlikely\" by determining whether or not the observed test statistic is more extreme than would be expected if the null hypothesis were true. That is, it entails comparing the observed test statistic to some cutoff value, called the \"critical value.\n",
    "\n",
    "# P-value approach\n",
    "- The P-value approach involves determining \"likely\" or \"unlikely\" by determining the probability — assuming the null hypothesis were true — of observing a more extreme test statistic in the direction of the alternative hypothesis than the one observed. If the P-value is small, say less than (or equal to) α, then it is \"unlikely.\" And, if the P-value is large, say more than α, then it is \"likely.\"\n",
    "\n",
    "# Chi-square test of independence\n",
    "The chi-square test is a statistical test of independence to determine the dependency of two variables. It shares similarities with coefficient of determination, $R^2$. \n",
    "- However, chi-square test is only applicable to categorical or nominal data while $R^2$ is only applicable to numeric data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
